{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emotion Detection with Machine Learning\n",
    "\n",
    "This notebook demonstrates how to build emotion classification models using text data. We'll train multiple models and evaluate their performance in classifying emotions like Happy, Sad, Angry, Neutral, Fear, and Surprise.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Setup and Imports](#setup)\n",
    "2. [Data Loading and Exploration](#data)\n",
    "3. [Data Preprocessing](#preprocessing)\n",
    "4. [Model Training](#training)\n",
    "5. [Model Evaluation](#evaluation)\n",
    "6. [Confusion Matrix Analysis](#confusion)\n",
    "7. [Making Predictions](#predictions)\n",
    "8. [Model Comparison](#comparison)\n",
    "9. [Error Analysis](#errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports <a id=\"setup\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Import our custom classes\n",
    "from emotion_classifier import EmotionClassifier\n",
    "from data_processor import DataProcessor\n",
    "from model_evaluator import ModelEvaluator\n",
    "from utils import *\n",
    "\n",
    "print(\"All libraries imported successfully!\")\n",
    "print(\"\\nChecking system requirements...\")\n",
    "requirements = check_system_requirements()\n",
    "for package, available in requirements.items():\n",
    "    status = \"✓\" if available else \"✗\"\n",
    "    print(f\"{status} {package}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Exploration <a id=\"data\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sample_dataset():\n",
    "    \"\"\"Create a sample emotion dataset for demonstration\"\"\"\n",
    "    emotions = ['Happy', 'Sad', 'Angry', 'Neutral', 'Fear', 'Surprise']\n",
    "    \n",
    "    # Sample texts for each emotion\n",
    "    sample_texts = {\n",
    "        'Happy': [\n",
    "            \"I'm so excited about this amazing opportunity!\",\n",
    "            \"What a beautiful sunny day, feeling fantastic!\",\n",
    "            \"Just got the best news ever, I'm thrilled!\",\n",
    "            \"Love spending time with my family and friends\",\n",
    "            \"This is the best day of my life!\",\n",
    "            \"I feel so grateful and blessed today\",\n",
    "            \"Amazing performance, I'm so proud!\",\n",
    "            \"Everything is going perfectly, I'm delighted!\",\n",
    "            \"Celebrating this wonderful achievement!\",\n",
    "            \"Feeling joy and happiness all around me\"\n",
    "        ],\n",
    "        'Sad': [\n",
    "            \"I'm feeling really down today\",\n",
    "            \"This is such a disappointing situation\",\n",
    "            \"I miss my old friends so much\",\n",
    "            \"Feeling lonely and isolated lately\",\n",
    "            \"Nothing seems to be going right\",\n",
    "            \"I'm having a really tough time\",\n",
    "            \"This news made me feel so heartbroken\",\n",
    "            \"I feel empty and lost right now\",\n",
    "            \"Tears are flowing down my face\",\n",
    "            \"Everything feels hopeless and dark\"\n",
    "        ],\n",
    "        'Angry': [\n",
    "            \"This is absolutely infuriating!\",\n",
    "            \"I can't believe how unfair this is\",\n",
    "            \"I'm so frustrated with this situation\",\n",
    "            \"This makes my blood boil!\",\n",
    "            \"I'm outraged by this behavior\",\n",
    "            \"This is completely unacceptable!\",\n",
    "            \"I'm fed up with all these problems\",\n",
    "            \"This injustice makes me furious!\",\n",
    "            \"I'm livid about what happened\",\n",
    "            \"This disrespect is making me mad\"\n",
    "        ],\n",
    "        'Neutral': [\n",
    "            \"The weather today is partly cloudy\",\n",
    "            \"I need to go to the grocery store\",\n",
    "            \"The meeting is scheduled for 3 PM\",\n",
    "            \"Please review the attached document\",\n",
    "            \"The report is due next Friday\",\n",
    "            \"I'll be working from home tomorrow\",\n",
    "            \"The conference will be held virtually\",\n",
    "            \"Please confirm your attendance\",\n",
    "            \"The office hours are 9 AM to 5 PM\",\n",
    "            \"The project deadline is next month\"\n",
    "        ],\n",
    "        'Fear': [\n",
    "            \"I'm terrified of what might happen\",\n",
    "            \"This situation makes me very anxious\",\n",
    "            \"I'm worried about the future\",\n",
    "            \"I feel scared and uncertain\",\n",
    "            \"This gives me chills down my spine\",\n",
    "            \"I'm afraid things will get worse\",\n",
    "            \"The thought of this terrifies me\",\n",
    "            \"I'm nervous about the outcome\",\n",
    "            \"This makes me feel vulnerable and helpless\",\n",
    "            \"I'm trembling with fear right now\"\n",
    "        ],\n",
    "        'Surprise': [\n",
    "            \"I can't believe this actually happened!\",\n",
    "            \"What a shocking turn of events!\",\n",
    "            \"I never expected this to occur\",\n",
    "            \"This is completely unexpected!\",\n",
    "            \"I'm amazed by this revelation\",\n",
    "            \"What a surprising discovery!\",\n",
    "            \"This caught me completely off guard\",\n",
    "            \"I'm stunned by this news!\",\n",
    "            \"Wow, this is absolutely incredible!\",\n",
    "            \"I'm blown away by this surprise\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # Create DataFrame\n",
    "    data = []\n",
    "    for emotion, texts in sample_texts.items():\n",
    "        for text in texts:\n",
    "            data.append({'text': text, 'emotion': emotion})\n",
    "    \n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Load the sample dataset\n",
    "df = create_sample_dataset()\n",
    "\n",
    "print(f\"Dataset loaded successfully!\")\n",
    "print(f\"Total samples: {len(df)}\")\n",
    "print(f\"Unique emotions: {df['emotion'].nunique()}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data exploration\n",
    "print(\"=== Dataset Statistics ===\")\n",
    "print(f\"Total samples: {len(df)}\")\n",
    "print(f\"Unique emotions: {df['emotion'].nunique()}\")\n",
    "print(f\"Average text length: {df['text'].str.len().mean():.1f} characters\")\n",
    "print(f\"Min text length: {df['text'].str.len().min()} characters\")\n",
    "print(f\"Max text length: {df['text'].str.len().max()} characters\")\n",
    "\n",
    "print(\"\\n=== Emotion Distribution ===\")\n",
    "emotion_counts = df['emotion'].value_counts()\n",
    "print(emotion_counts)\n",
    "\n",
    "# Visualize emotion distribution\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Bar plot\n",
    "emotion_counts.plot(kind='bar', ax=ax1, color='skyblue')\n",
    "ax1.set_title('Emotion Distribution')\n",
    "ax1.set_xlabel('Emotion')\n",
    "ax1.set_ylabel('Count')\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Pie chart\n",
    "ax2.pie(emotion_counts.values, labels=emotion_counts.index, autopct='%1.1f%%')\n",
    "ax2.set_title('Emotion Distribution (Percentage)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text length analysis\n",
    "df['text_length'] = df['text'].str.len()\n",
    "df['word_count'] = df['text'].str.split().str.len()\n",
    "\n",
    "# Box plots for text length by emotion\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Text length distribution by emotion\n",
    "df.boxplot(column='text_length', by='emotion', ax=ax1)\n",
    "ax1.set_title('Text Length Distribution by Emotion')\n",
    "ax1.set_xlabel('Emotion')\n",
    "ax1.set_ylabel('Text Length (characters)')\n",
    "\n",
    "# Word count distribution by emotion\n",
    "df.boxplot(column='word_count', by='emotion', ax=ax2)\n",
    "ax2.set_title('Word Count Distribution by Emotion')\n",
    "ax2.set_xlabel('Emotion')\n",
    "ax2.set_ylabel('Word Count')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary statistics\n",
    "print(\"\\n=== Text Length Statistics by Emotion ===\")\n",
    "text_stats = df.groupby('emotion')['text_length'].agg(['mean', 'std', 'min', 'max']).round(2)\n",
    "print(text_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing <a id=\"preprocessing\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize data processor\n",
    "processor = DataProcessor()\n",
    "\n",
    "# Show original vs processed text examples\n",
    "print(\"=== Text Preprocessing Examples ===\")\n",
    "sample_texts = df['text'].head(5).tolist()\n",
    "\n",
    "for i, text in enumerate(sample_texts):\n",
    "    processed = processor.preprocess_text(text)\n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(f\"Original:  {text}\")\n",
    "    print(f\"Processed: {processed}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the entire dataset\n",
    "print(\"Preprocessing the dataset...\")\n",
    "df_processed = processor.preprocess_data(df.copy())\n",
    "\n",
    "print(f\"\\nDataset size before preprocessing: {len(df)}\")\n",
    "print(f\"Dataset size after preprocessing: {len(df_processed)}\")\n",
    "\n",
    "# Show the processed dataset\n",
    "print(\"\\n=== Processed Dataset Sample ===\")\n",
    "print(df_processed.head(10))\n",
    "\n",
    "# Get text statistics\n",
    "stats = processor.get_text_statistics(df_processed)\n",
    "print(\"\\n=== Text Statistics (After Preprocessing) ===\")\n",
    "for key, value in stats.items():\n",
    "    print(f\"{key}: {value:.2f}\" if isinstance(value, float) else f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Training <a id=\"training\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the emotion classifier\n",
    "classifier = EmotionClassifier()\n",
    "\n",
    "# Training parameters\n",
    "training_params = {\n",
    "    'test_size': 0.2,\n",
    "    'random_state': 42,\n",
    "    'max_features': 5000,\n",
    "    'ngram_range': (1, 2),\n",
    "    'models_to_train': [\"Naive Bayes\", \"SVM\", \"Random Forest\", \"Logistic Regression\"]\n",
    "}\n",
    "\n",
    "print(\"=== Training Parameters ===\")\n",
    "for param, value in training_params.items():\n",
    "    print(f\"{param}: {value}\")\n",
    "\n",
    "print(\"\\n=== Starting Model Training ===\")\n",
    "print(\"This may take a few minutes...\")\n",
    "\n",
    "# Train the models\n",
    "training_results = classifier.train_models(df_processed, **training_params)\n",
    "\n",
    "print(\"\\n=== Training Results ===\")\n",
    "results_df = pd.DataFrame(training_results).T\n",
    "results_df = results_df.sort_values('accuracy', ascending=False)\n",
    "print(results_df)\n",
    "\n",
    "# Find best model\n",
    "best_model = results_df.index[0]\n",
    "best_accuracy = results_df.loc[best_model, 'accuracy']\n",
    "print(f\"\\n🏆 Best Model: {best_model} (Accuracy: {best_accuracy:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training results\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Accuracy comparison\n",
    "models = list(training_results.keys())\n",
    "accuracies = [training_results[model]['accuracy'] for model in models]\n",
    "training_times = [training_results[model]['training_time'] for model in models]\n",
    "\n",
    "bars1 = ax1.bar(models, accuracies, color='lightblue', alpha=0.7)\n",
    "ax1.set_title('Model Accuracy Comparison')\n",
    "ax1.set_ylabel('Accuracy')\n",
    "ax1.set_ylim(0, 1)\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Add accuracy values on bars\n",
    "for bar, acc in zip(bars1, accuracies):\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "             f'{acc:.3f}', ha='center', va='bottom')\n",
    "\n",
    "# Training time comparison\n",
    "bars2 = ax2.bar(models, training_times, color='lightcoral', alpha=0.7)\n",
    "ax2.set_title('Training Time Comparison')\n",
    "ax2.set_ylabel('Training Time (seconds)')\n",
    "ax2.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Add time values on bars\n",
    "for bar, time in zip(bars2, training_times):\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "             f'{time:.2f}s', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Evaluation <a id=\"evaluation\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model evaluator\n",
    "evaluator = ModelEvaluator(classifier)\n",
    "\n",
    "# Evaluate all models\n",
    "print(\"=== Detailed Model Evaluation ===\")\n",
    "all_evaluations = {}\n",
    "\n",
    "for model_name in training_results.keys():\n",
    "    print(f\"\\n--- Evaluating {model_name} ---\")\n",
    "    evaluation = evaluator.evaluate_model(model_name)\n",
    "    all_evaluations[model_name] = evaluation\n",
    "    \n",
    "    print(f\"Accuracy:  {evaluation['accuracy']:.4f}\")\n",
    "    print(f\"Precision: {evaluation['precision']:.4f}\")\n",
    "    print(f\"Recall:    {evaluation['recall']:.4f}\")\n",
    "    print(f\"F1-Score:  {evaluation['f1_score']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all models\n",
    "comparison_df = evaluator.compare_models()\n",
    "print(\"=== Model Comparison Summary ===\")\n",
    "print(comparison_df)\n",
    "\n",
    "# Visualize model comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
    "colors = ['lightblue', 'lightgreen', 'lightcoral', 'lightyellow']\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    "    bars = axes[i].bar(comparison_df['Model'], comparison_df[metric], \n",
    "                      color=colors[i], alpha=0.7)\n",
    "    axes[i].set_title(f'{metric} Comparison')\n",
    "    axes[i].set_ylabel(metric)\n",
    "    axes[i].set_ylim(0, 1)\n",
    "    axes[i].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Add values on bars\n",
    "    for bar, value in zip(bars, comparison_df[metric]):\n",
    "        axes[i].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                    f'{value:.3f}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Confusion Matrix Analysis <a id=\"confusion\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrices for all models\n",
    "n_models = len(training_results)\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, (model_name, evaluation) in enumerate(all_evaluations.items()):\n",
    "    cm = evaluation['confusion_matrix']\n",
    "    labels = evaluation['labels']\n",
    "    \n",
    "    # Normalize confusion matrix\n",
    "    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    im = axes[i].imshow(cm_normalized, interpolation='nearest', cmap='Blues')\n",
    "    axes[i].set_title(f'Confusion Matrix - {model_name}\\n(Normalized)')\n",
    "    \n",
    "    # Add labels\n",
    "    axes[i].set_xticks(range(len(labels)))\n",
    "    axes[i].set_yticks(range(len(labels)))\n",
    "    axes[i].set_xticklabels(labels, rotation=45)\n",
    "    axes[i].set_yticklabels(labels)\n",
    "    axes[i].set_xlabel('Predicted')\n",
    "    axes[i].set_ylabel('Actual')\n",
    "    \n",
    "    # Add text annotations\n",
    "    for j in range(len(labels)):\n",
    "        for k in range(len(labels)):\n",
    "            text = axes[i].text(k, j, f'{cm_normalized[j, k]:.2f}',\n",
    "                               ha=\"center\", va=\"center\", color=\"black\")\n",
    "    \n",
    "    # Add colorbar\n",
    "    plt.colorbar(im, ax=axes[i], fraction=0.046, pad=0.04)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed classification report for the best model\n",
    "best_model_name = comparison_df.iloc[0]['Model']\n",
    "print(f\"=== Detailed Classification Report for {best_model_name} ===\")\n",
    "\n",
    "detailed_report = evaluator.get_detailed_classification_report(best_model_name)\n",
    "print(detailed_report)\n",
    "\n",
    "# Per-emotion performance\n",
    "emotion_performance = evaluator.get_per_emotion_performance(best_model_name)\n",
    "print(f\"\\n=== Per-Emotion Performance ({best_model_name}) ===\")\n",
    "print(emotion_performance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Making Predictions <a id=\"predictions\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test predictions with sample texts\n",
    "test_texts = [\n",
    "    \"I'm absolutely thrilled about this wonderful news!\",\n",
    "    \"This situation is making me really upset and angry\",\n",
    "    \"I feel so lonely and depressed today\",\n",
    "    \"The meeting is scheduled for tomorrow at 2 PM\",\n",
    "    \"I'm terrified about what might happen next\",\n",
    "    \"Wow, I never saw that coming!\"\n",
    "]\n",
    "\n",
    "expected_emotions = ['Happy', 'Angry', 'Sad', 'Neutral', 'Fear', 'Surprise']\n",
    "\n",
    "print(\"=== Prediction Examples ===\")\n",
    "print(f\"Using best model: {best_model_name}\\n\")\n",
    "\n",
    "predictions_results = []\n",
    "\n",
    "for i, text in enumerate(test_texts):\n",
    "    prediction, probabilities = classifier.predict_single(text, best_model_name)\n",
    "    \n",
    "    print(f\"Text {i+1}: {text}\")\n",
    "    print(f\"Predicted: {prediction}\")\n",
    "    print(f\"Expected:  {expected_emotions[i]}\")\n",
    "    print(f\"Correct:   {'✓' if prediction == expected_emotions[i] else '✗'}\")\n",
    "    \n",
    "    # Show top 3 probabilities\n",
    "    sorted_probs = sorted(probabilities.items(), key=lambda x: x[1], reverse=True)\n",
    "    print(\"Top 3 predictions:\")\n",
    "    for emotion, prob in sorted_probs[:3]:\n",
    "        print(f\"  {emotion}: {prob:.3f}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    predictions_results.append({\n",
    "        'text': text,\n",
    "        'predicted': prediction,\n",
    "        'expected': expected_emotions[i],\n",
    "        'correct': prediction == expected_emotions[i],\n",
    "        'confidence': max(probabilities.values())\n",
    "    })\n",
    "\n",
    "# Summary\n",
    "correct_predictions = sum(1 for r in predictions_results if r['correct'])\n",
    "accuracy = correct_predictions / len(predictions_results)\n",
    "print(f\"\\nPrediction Accuracy: {accuracy:.2%} ({correct_predictions}/{len(predictions_results)})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive prediction function\n",
    "def predict_emotion_interactive(text, model_name=None):\n",
    "    \"\"\"Make prediction and show detailed results\"\"\"\n",
    "    if model_name is None:\n",
    "        model_name = best_model_name\n",
    "    \n",
    "    prediction, probabilities = classifier.predict_single(text, model_name)\n",
    "    \n",
    "    print(f\"Input Text: {text}\")\n",
    "    print(f\"Predicted Emotion: {prediction}\")\n",
    "    print(f\"\\nConfidence Scores:\")\n",
    "    \n",
    "    # Sort probabilities\n",
    "    sorted_probs = sorted(probabilities.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    for emotion, prob in sorted_probs:\n",
    "        bar = '█' * int(prob * 20)\n",
    "        print(f\"  {emotion:10}: {prob:.3f} {bar}\")\n",
    "    \n",
    "    return prediction, probabilities\n",
    "\n",
    "# Example usage\n",
    "print(\"=== Interactive Prediction Example ===\")\n",
    "sample_text = \"I can't believe I won the lottery! This is incredible!\"\n",
    "predict_emotion_interactive(sample_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Model Comparison <a id=\"comparison\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare predictions across all models for the same text\n",
    "comparison_text = \"I'm really worried and scared about the future\"\n",
    "\n",
    "print(f\"=== Model Comparison for Text ===\")\n",
    "print(f\"Text: {comparison_text}\\n\")\n",
    "\n",
    "model_predictions = {}\n",
    "\n",
    "for model_name in training_results.keys():\n",
    "    prediction, probabilities = classifier.predict_single(comparison_text, model_name)\n",
    "    model_predictions[model_name] = {\n",
    "        'prediction': prediction,\n",
    "        'confidence': max(probabilities.values()),\n",
    "        'probabilities': probabilities\n",
    "    }\n",
    "    \n",
    "    print(f\"{model_name}:\")\n",
    "    print(f\"  Prediction: {prediction}\")\n",
    "    print(f\"  Confidence: {max(probabilities.values()):.3f}\")\n",
    "    print()\n",
    "\n",
    "# Visualize prediction comparison\n",
    "emotions = list(classifier.emotion_labels)\n",
    "n_models = len(model_predictions)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "x = np.arange(len(emotions))\n",
    "width = 0.2\n",
    "\n",
    "for i, (model_name, results) in enumerate(model_predictions.items()):\n",
    "    probs = [results['probabilities'][emotion] for emotion in emotions]\n",
    "    ax.bar(x + i * width, probs, width, label=model_name, alpha=0.8)\n",
    "\n",
    "ax.set_xlabel('Emotions')\n",
    "ax.set_ylabel('Probability')\n",
    "ax.set_title(f'Model Predictions Comparison\\nText: \"{comparison_text}\"')\n",
    "ax.set_xticks(x + width * (n_models - 1) / 2)\n",
    "ax.set_xticklabels(emotions)\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Error Analysis <a id=\"errors\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze misclassified examples\n",
    "print(f\"=== Error Analysis for {best_model_name} ===\")\n",
    "\n",
    "misclassified = evaluator.get_misclassified_examples(best_model_name, num_examples=10)\n",
    "\n",
    "if len(misclassified) > 0:\n",
    "    print(f\"Found {len(misclassified)} misclassified examples:\\n\")\n",
    "    \n",
    "    for i, row in misclassified.iterrows():\n",
    "        print(f\"Example {i+1}:\")\n",
    "        print(f\"  Text: {row['Text']}\")\n",
    "        print(f\"  True Emotion: {row['True_Emotion']}\")\n",
    "        print(f\"  Predicted: {row['Predicted_Emotion']}\")\n",
    "        print()\n",
    "else:\n",
    "    print(\"No misclassified examples found! Perfect classification.\")\n",
    "\n",
    "# Error pattern analysis\n",
    "evaluation = all_evaluations[best_model_name]\n",
    "y_true = evaluation['y_true']\n",
    "y_pred = evaluation['y_pred']\n",
    "\n",
    "# Create error matrix\n",
    "error_matrix = pd.crosstab(pd.Series(y_true, name='Actual'), \n",
    "                          pd.Series(y_pred, name='Predicted'), \n",
    "                          margins=True)\n",
    "\n",
    "print(f\"\\n=== Error Matrix ({best_model_name}) ===\")\n",
    "print(error_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model performance summary\n",
    "performance_summary = evaluator.get_model_performance_summary()\n",
    "\n",
    "print(\"=== Final Model Performance Summary ===\")\n",
    "print(f\"Best Model: {performance_summary['best_model']['name']}\")\n",
    "print(f\"Best Accuracy: {performance_summary['best_model']['accuracy']:.4f}\")\n",
    "\n",
    "print(\"\\nAll Models Performance:\")\n",
    "summary_df = pd.DataFrame({k: v for k, v in performance_summary.items() \n",
    "                          if k != 'best_model'}).T\n",
    "summary_df = summary_df.sort_values('accuracy', ascending=False)\n",
    "print(summary_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook demonstrated a complete emotion classification pipeline:\n",
    "\n",
    "1. **Data Loading**: Created a balanced dataset with 6 emotion categories\n",
    "2. **Preprocessing**: Cleaned and processed text data using NLTK\n",
    "3. **Model Training**: Trained 4 different ML models with TF-IDF features\n",
    "4. **Evaluation**: Comprehensive evaluation with multiple metrics\n",
    "5. **Analysis**: Confusion matrices, error analysis, and model comparison\n",
    "\n",
    "### Key Findings:\n",
    "- Best performing model achieved **X.XX%** accuracy\n",
    "- Some emotions are easier to classify than others\n",
    "- Text preprocessing significantly impacts performance\n",
    "- Different models have different strengths and weaknesses\n",
    "\n",
    "### Next Steps:\n",
    "1. Try more advanced models (BERT, RoBERTa)\n",
    "2. Collect more training data\n",
    "3. Implement ensemble methods\n",
    "4. Add more sophisticated text features\n",
    "5. Deploy the model as a web service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained models and results\n",
    "print(\"=== Saving Results ===\")\n",
    "\n",
    "# Save models\n",
    "classifier.save_models('emotion_classifier_models.pkl')\n",
    "print(\"✓ Models saved to 'emotion_classifier_models.pkl'\")\n",
    "\n",
    "# Save evaluation results\n",
    "results_filename = save_results_to_csv(performance_summary)\n",
    "print(f\"✓ Results saved to '{results_filename}'\")\n",
    "\n",
    "# Export model summary\n",
    "summary_filename = export_model_summary(classifier)\n",
    "print(f\"✓ Model summary saved to '{summary_filename}'\")\n",
    "\n",
    "print(\"\\nAll results have been saved successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}