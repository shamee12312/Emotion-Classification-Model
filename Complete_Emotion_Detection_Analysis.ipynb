{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complete Emotion Detection Analysis\n",
    "\n",
    "## End-to-End Machine Learning Pipeline for Text Emotion Classification\n",
    "\n",
    "This notebook demonstrates a comprehensive machine learning pipeline for emotion detection in text data, covering:\n",
    "\n",
    "1. **Data Exploration & Analysis (EDA)**\n",
    "2. **Feature Engineering & Preprocessing**\n",
    "3. **Model Training & Selection**\n",
    "4. **Model Evaluation & Metrics**\n",
    "5. **Model Deployment & Inference**\n",
    "\n",
    "### Dataset Overview\n",
    "- **Total Samples**: 240 diverse text examples\n",
    "- **Emotions**: Happy, Sad, Angry, Neutral, Fear, Surprise (40 samples each)\n",
    "- **Text Variety**: Real-world scenarios, different contexts and expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Machine Learning libraries\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    classification_report, confusion_matrix, roc_curve, auc\n",
    ")\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Text preprocessing\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Download NLTK data\n",
    "try:\n",
    "    nltk.download('punkt', quiet=True)\n",
    "    nltk.download('stopwords', quiet=True)\n",
    "    nltk.download('wordnet', quiet=True)\n",
    "    nltk.download('omw-1.4', quiet=True)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"All libraries imported successfully!\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the comprehensive emotion dataset\n",
    "df = pd.read_csv('comprehensive_emotion_dataset.csv')\n",
    "\n",
    "print(f\"Dataset loaded successfully!\")\n",
    "print(f\"Total samples: {len(df)}\")\n",
    "print(f\"Unique emotions: {df['emotion'].nunique()}\")\n",
    "print(f\"Emotions: {sorted(df['emotion'].unique())}\")\n",
    "\n",
    "# Display first few rows\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset statistics\n",
    "print(\"=== Dataset Statistics ===\")\n",
    "print(f\"Total samples: {len(df)}\")\n",
    "print(f\"Unique emotions: {df['emotion'].nunique()}\")\n",
    "print(f\"Average text length: {df['text'].str.len().mean():.1f} characters\")\n",
    "print(f\"Min text length: {df['text'].str.len().min()} characters\")\n",
    "print(f\"Max text length: {df['text'].str.len().max()} characters\")\n",
    "print(f\"Median text length: {df['text'].str.len().median():.1f} characters\")\n",
    "\n",
    "# Check for missing values\n",
    "print(f\"\\nMissing values:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Emotion distribution\n",
    "print(\"\\n=== Emotion Distribution ===\")\n",
    "emotion_counts = df['emotion'].value_counts()\n",
    "emotion_percentages = df['emotion'].value_counts(normalize=True) * 100\n",
    "\n",
    "distribution_df = pd.DataFrame({\n",
    "    'Count': emotion_counts,\n",
    "    'Percentage': emotion_percentages\n",
    "})\n",
    "print(distribution_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize emotion distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Bar plot\n",
    "emotion_counts.plot(kind='bar', ax=axes[0], color='skyblue', alpha=0.8)\n",
    "axes[0].set_title('Emotion Distribution (Counts)', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Emotion', fontsize=12)\n",
    "axes[0].set_ylabel('Count', fontsize=12)\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Add count labels on bars\n",
    "for i, v in enumerate(emotion_counts.values):\n",
    "    axes[0].text(i, v + 0.5, str(v), ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Pie chart\n",
    "colors = plt.cm.Set3(np.linspace(0, 1, len(emotion_counts)))\n",
    "axes[1].pie(emotion_counts.values, labels=emotion_counts.index, autopct='%1.1f%%', \n",
    "           colors=colors, startangle=90)\n",
    "axes[1].set_title('Emotion Distribution (Percentages)', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Check if dataset is balanced\n",
    "balance_check = emotion_counts.std() / emotion_counts.mean()\n",
    "print(f\"\\nDataset Balance Check:\")\n",
    "print(f\"Standard deviation / Mean ratio: {balance_check:.4f}\")\n",
    "print(f\"Dataset is {'balanced' if balance_check < 0.1 else 'imbalanced'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text length analysis\n",
    "df['text_length'] = df['text'].str.len()\n",
    "df['word_count'] = df['text'].str.split().str.len()\n",
    "\n",
    "# Statistical summary by emotion\n",
    "print(\"=== Text Length Statistics by Emotion ===\")\n",
    "text_stats = df.groupby('emotion')['text_length'].agg(['mean', 'std', 'min', 'max', 'median']).round(2)\n",
    "print(text_stats)\n",
    "\n",
    "print(\"\\n=== Word Count Statistics by Emotion ===\")\n",
    "word_stats = df.groupby('emotion')['word_count'].agg(['mean', 'std', 'min', 'max', 'median']).round(2)\n",
    "print(word_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize text length distributions\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Text length by emotion (box plot)\n",
    "df.boxplot(column='text_length', by='emotion', ax=axes[0,0])\n",
    "axes[0,0].set_title('Text Length Distribution by Emotion')\n",
    "axes[0,0].set_xlabel('Emotion')\n",
    "axes[0,0].set_ylabel('Text Length (characters)')\n",
    "axes[0,0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Word count by emotion (box plot)\n",
    "df.boxplot(column='word_count', by='emotion', ax=axes[0,1])\n",
    "axes[0,1].set_title('Word Count Distribution by Emotion')\n",
    "axes[0,1].set_xlabel('Emotion')\n",
    "axes[0,1].set_ylabel('Word Count')\n",
    "axes[0,1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Overall text length histogram\n",
    "axes[1,0].hist(df['text_length'], bins=30, alpha=0.7, color='lightblue', edgecolor='black')\n",
    "axes[1,0].set_title('Overall Text Length Distribution')\n",
    "axes[1,0].set_xlabel('Text Length (characters)')\n",
    "axes[1,0].set_ylabel('Frequency')\n",
    "axes[1,0].grid(True, alpha=0.3)\n",
    "\n",
    "# Overall word count histogram\n",
    "axes[1,1].hist(df['word_count'], bins=20, alpha=0.7, color='lightgreen', edgecolor='black')\n",
    "axes[1,1].set_title('Overall Word Count Distribution')\n",
    "axes[1,1].set_xlabel('Word Count')\n",
    "axes[1,1].set_ylabel('Frequency')\n",
    "axes[1,1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Text Preprocessing and Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text preprocessing class\n",
    "class TextPreprocessor:\n",
    "    def __init__(self):\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        try:\n",
    "            self.stop_words = set(stopwords.words('english'))\n",
    "        except:\n",
    "            self.stop_words = set()\n",
    "    \n",
    "    def clean_text(self, text):\n",
    "        \"\"\"Clean and preprocess text\"\"\"\n",
    "        if pd.isna(text) or not isinstance(text, str):\n",
    "            return \"\"\n",
    "        \n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "        \n",
    "        # Remove URLs and email addresses\n",
    "        text = re.sub(r'http\\S+|www.\\S+|\\S+@\\S+', '', text)\n",
    "        \n",
    "        # Remove mentions and hashtags\n",
    "        text = re.sub(r'@\\w+|#\\w+', '', text)\n",
    "        \n",
    "        # Keep emotion-related punctuation (! and ?) but remove others\n",
    "        text = re.sub(r'[^\\w\\s!?]', ' ', text)\n",
    "        \n",
    "        # Normalize multiple exclamation/question marks\n",
    "        text = re.sub(r'!+', '!', text)\n",
    "        text = re.sub(r'\\?+', '?', text)\n",
    "        \n",
    "        # Remove extra whitespace\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        \n",
    "        return text.strip()\n",
    "    \n",
    "    def tokenize_and_lemmatize(self, text):\n",
    "        \"\"\"Tokenize and lemmatize text\"\"\"\n",
    "        try:\n",
    "            tokens = word_tokenize(text)\n",
    "            processed_tokens = []\n",
    "            \n",
    "            for token in tokens:\n",
    "                if token.lower() not in self.stop_words and len(token) > 2:\n",
    "                    try:\n",
    "                        lemmatized = self.lemmatizer.lemmatize(token.lower())\n",
    "                        processed_tokens.append(lemmatized)\n",
    "                    except:\n",
    "                        processed_tokens.append(token.lower())\n",
    "            \n",
    "            return ' '.join(processed_tokens)\n",
    "        except:\n",
    "            # Fallback if NLTK processing fails\n",
    "            words = text.split()\n",
    "            filtered_words = [word for word in words if len(word) > 2]\n",
    "            return ' '.join(filtered_words)\n",
    "    \n",
    "    def preprocess(self, text):\n",
    "        \"\"\"Complete preprocessing pipeline\"\"\"\n",
    "        cleaned = self.clean_text(text)\n",
    "        processed = self.tokenize_and_lemmatize(cleaned)\n",
    "        return processed\n",
    "\n",
    "# Initialize preprocessor\n",
    "preprocessor = TextPreprocessor()\n",
    "\n",
    "# Show preprocessing examples\n",
    "print(\"=== Text Preprocessing Examples ===\")\n",
    "sample_texts = [\n",
    "    \"I'm absolutely thrilled about this promotion!\",\n",
    "    \"This is completely unacceptable!!!\",\n",
    "    \"I feel so sad and lonely today...\",\n",
    "    \"The meeting is scheduled for 2 PM tomorrow.\",\n",
    "    \"I'm terrified of what might happen next???\"\n",
    "]\n",
    "\n",
    "for i, text in enumerate(sample_texts, 1):\n",
    "    processed = preprocessor.preprocess(text)\n",
    "    print(f\"\\nExample {i}:\")\n",
    "    print(f\"Original:  {text}\")\n",
    "    print(f\"Processed: {processed}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply preprocessing to the entire dataset\n",
    "print(\"Preprocessing the entire dataset...\")\n",
    "df['processed_text'] = df['text'].apply(preprocessor.preprocess)\n",
    "\n",
    "# Remove empty processed texts\n",
    "df_clean = df[df['processed_text'].str.strip() != ''].copy()\n",
    "\n",
    "print(f\"Dataset size before preprocessing: {len(df)}\")\n",
    "print(f\"Dataset size after preprocessing: {len(df_clean)}\")\n",
    "\n",
    "# Show processed dataset sample\n",
    "print(\"\\n=== Processed Dataset Sample ===\")\n",
    "display_df = df_clean[['text', 'processed_text', 'emotion']].head(10)\n",
    "for idx, row in display_df.iterrows():\n",
    "    print(f\"\\nEmotion: {row['emotion']}\")\n",
    "    print(f\"Original: {row['text']}\")\n",
    "    print(f\"Processed: {row['processed_text']}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Extraction and Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features and labels\n",
    "X = df_clean['processed_text']\n",
    "y = df_clean['emotion']\n",
    "\n",
    "# Check class distribution after preprocessing\n",
    "print(\"=== Class Distribution After Preprocessing ===\")\n",
    "class_dist = y.value_counts()\n",
    "print(class_dist)\n",
    "\n",
    "# Split the data\n",
    "test_size = 0.25\n",
    "random_state = 42\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=test_size, random_state=random_state, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"\\n=== Data Split ===\")\n",
    "print(f\"Training set size: {len(X_train)} ({len(X_train)/len(X)*100:.1f}%)\")\n",
    "print(f\"Test set size: {len(X_test)} ({len(X_test)/len(X)*100:.1f}%)\")\n",
    "\n",
    "# Check class distribution in train/test sets\n",
    "print(f\"\\nTraining set class distribution:\")\n",
    "print(y_train.value_counts())\n",
    "print(f\"\\nTest set class distribution:\")\n",
    "print(y_test.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature extraction with TF-IDF\n",
    "# Compare different vectorization approaches\n",
    "\n",
    "vectorizers = {\n",
    "    'TF-IDF (1-2 grams)': TfidfVectorizer(\n",
    "        max_features=5000,\n",
    "        ngram_range=(1, 2),\n",
    "        min_df=1,\n",
    "        max_df=0.95,\n",
    "        sublinear_tf=True\n",
    "    ),\n",
    "    'TF-IDF (1 gram)': TfidfVectorizer(\n",
    "        max_features=3000,\n",
    "        ngram_range=(1, 1),\n",
    "        min_df=1,\n",
    "        max_df=0.95\n",
    "    ),\n",
    "    'Count Vectorizer': CountVectorizer(\n",
    "        max_features=3000,\n",
    "        ngram_range=(1, 2),\n",
    "        min_df=1,\n",
    "        max_df=0.95\n",
    "    )\n",
    "}\n",
    "\n",
    "# Test vectorizers\n",
    "print(\"=== Feature Extraction Comparison ===\")\n",
    "vectorizer_results = {}\n",
    "\n",
    "for name, vectorizer in vectorizers.items():\n",
    "    # Fit and transform\n",
    "    X_train_vec = vectorizer.fit_transform(X_train)\n",
    "    X_test_vec = vectorizer.transform(X_test)\n",
    "    \n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  Feature matrix shape: {X_train_vec.shape}\")\n",
    "    print(f\"  Sparsity: {(1 - X_train_vec.nnz / (X_train_vec.shape[0] * X_train_vec.shape[1])):.4f}\")\n",
    "    \n",
    "    vectorizer_results[name] = {\n",
    "        'vectorizer': vectorizer,\n",
    "        'X_train': X_train_vec,\n",
    "        'X_test': X_test_vec\n",
    "    }\n",
    "\n",
    "# Select the best vectorizer (TF-IDF with 1-2 grams)\n",
    "best_vectorizer_name = 'TF-IDF (1-2 grams)'\n",
    "vectorizer = vectorizer_results[best_vectorizer_name]['vectorizer']\n",
    "X_train_vectorized = vectorizer_results[best_vectorizer_name]['X_train']\n",
    "X_test_vectorized = vectorizer_results[best_vectorizer_name]['X_test']\n",
    "\n",
    "print(f\"\\nSelected vectorizer: {best_vectorizer_name}\")\n",
    "print(f\"Final feature matrix shape: {X_train_vectorized.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Training and Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models with optimized parameters\n",
    "models = {\n",
    "    'Naive Bayes': MultinomialNB(alpha=0.1),\n",
    "    'SVM': SVC(kernel='linear', C=1.0, probability=True, random_state=random_state),\n",
    "    'Random Forest': RandomForestClassifier(\n",
    "        n_estimators=200, \n",
    "        max_depth=20, \n",
    "        min_samples_split=5, \n",
    "        random_state=random_state\n",
    "    ),\n",
    "    'Logistic Regression': LogisticRegression(\n",
    "        C=1.0, \n",
    "        random_state=random_state, \n",
    "        max_iter=2000, \n",
    "        solver='liblinear'\n",
    "    )\n",
    "}\n",
    "\n",
    "# Train and evaluate models\n",
    "print(\"=== Model Training and Evaluation ===\")\n",
    "results = {}\n",
    "trained_models = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    \n",
    "    # Train model\n",
    "    start_time = time.time()\n",
    "    model.fit(X_train_vectorized, y_train)\n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test_vectorized)\n",
    "    y_pred_proba = None\n",
    "    \n",
    "    if hasattr(model, 'predict_proba'):\n",
    "        y_pred_proba = model.predict_proba(X_test_vectorized)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    \n",
    "    # Cross-validation\n",
    "    cv_scores = cross_val_score(model, X_train_vectorized, y_train, cv=5, scoring='accuracy')\n",
    "    \n",
    "    # Store results\n",
    "    results[name] = {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1,\n",
    "        'training_time': training_time,\n",
    "        'cv_mean': cv_scores.mean(),\n",
    "        'cv_std': cv_scores.std(),\n",
    "        'y_pred': y_pred,\n",
    "        'y_pred_proba': y_pred_proba\n",
    "    }\n",
    "    \n",
    "    trained_models[name] = model\n",
    "    \n",
    "    print(f\"  Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"  F1-Score: {f1:.4f}\")\n",
    "    print(f\"  CV Score: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")\n",
    "    print(f\"  Training Time: {training_time:.2f}s\")\n",
    "\n",
    "print(\"\\nModel training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Train and evaluate models (re-run the training cell above first)\n",
    "# This ensures all models are properly trained\n",
    "\n",
    "# Create comprehensive results summary\n",
    "results_df = pd.DataFrame(results).T\n",
    "results_df = results_df.round(4)\n",
    "\n",
    "print(\"=== Model Performance Summary ===\")\n",
    "print(results_df[['accuracy', 'precision', 'recall', 'f1_score', 'cv_mean', 'training_time']])\n",
    "\n",
    "# Find best model\n",
    "best_model_name = results_df['accuracy'].idxmax()\n",
    "best_accuracy = results_df.loc[best_model_name, 'accuracy']\n",
    "\n",
    "print(f\"\\n🏆 Best Model: {best_model_name}\")\n",
    "print(f\"   Accuracy: {best_accuracy:.4f}\")\n",
    "print(f\"   F1-Score: {results_df.loc[best_model_name, 'f1_score']:.4f}\")\n",
    "print(f\"   CV Score: {results_df.loc[best_model_name, 'cv_mean']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Evaluation and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model performance comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Accuracy comparison\n",
    "models_names = list(results.keys())\n",
    "accuracies = [results[model]['accuracy'] for model in models_names]\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, len(models_names)))\n",
    "\n",
    "bars1 = axes[0,0].bar(models_names, accuracies, color=colors, alpha=0.8)\n",
    "axes[0,0].set_title('Model Accuracy Comparison', fontsize=14, fontweight='bold')\n",
    "axes[0,0].set_ylabel('Accuracy')\n",
    "axes[0,0].set_ylim(0, 1)\n",
    "axes[0,0].tick_params(axis='x', rotation=45)\n",
    "axes[0,0].grid(True, alpha=0.3)\n",
    "\n",
    "# Add accuracy values on bars\n",
    "for bar, acc in zip(bars1, accuracies):\n",
    "    axes[0,0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                  f'{acc:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# F1-Score comparison\n",
    "f1_scores = [results[model]['f1_score'] for model in models_names]\n",
    "bars2 = axes[0,1].bar(models_names, f1_scores, color=colors, alpha=0.8)\n",
    "axes[0,1].set_title('F1-Score Comparison', fontsize=14, fontweight='bold')\n",
    "axes[0,1].set_ylabel('F1-Score')\n",
    "axes[0,1].set_ylim(0, 1)\n",
    "axes[0,1].tick_params(axis='x', rotation=45)\n",
    "axes[0,1].grid(True, alpha=0.3)\n",
    "\n",
    "for bar, f1 in zip(bars2, f1_scores):\n",
    "    axes[0,1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                  f'{f1:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Training time comparison\n",
    "training_times = [results[model]['training_time'] for model in models_names]\n",
    "bars3 = axes[1,0].bar(models_names, training_times, color='coral', alpha=0.8)\n",
    "axes[1,0].set_title('Training Time Comparison', fontsize=14, fontweight='bold')\n",
    "axes[1,0].set_ylabel('Training Time (seconds)')\n",
    "axes[1,0].tick_params(axis='x', rotation=45)\n",
    "axes[1,0].grid(True, alpha=0.3)\n",
    "\n",
    "for bar, time_val in zip(bars3, training_times):\n",
    "    axes[1,0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.001,\n",
    "                  f'{time_val:.3f}s', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Cross-validation scores\n",
    "cv_means = [results[model]['cv_mean'] for model in models_names]\n",
    "cv_stds = [results[model]['cv_std'] for model in models_names]\n",
    "bars4 = axes[1,1].bar(models_names, cv_means, yerr=cv_stds, color='lightgreen', \n",
    "                     alpha=0.8, capsize=5)\n",
    "axes[1,1].set_title('Cross-Validation Scores', fontsize=14, fontweight='bold')\n",
    "axes[1,1].set_ylabel('CV Accuracy')\n",
    "axes[1,1].set_ylim(0, 1)\n",
    "axes[1,1].tick_params(axis='x', rotation=45)\n",
    "axes[1,1].grid(True, alpha=0.3)\n",
    "\n",
    "for bar, cv_mean in zip(bars4, cv_means):\n",
    "    axes[1,1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                  f'{cv_mean:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed evaluation of the best model\n",
    "best_model = trained_models[best_model_name]\n",
    "y_pred_best = results[best_model_name]['y_pred']\n",
    "\n",
    "print(f\"=== Detailed Evaluation: {best_model_name} ===\")\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_best))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred_best)\n",
    "emotions = sorted(y_test.unique())\n",
    "\n",
    "# Plot confusion matrices\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Raw confusion matrix\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "           xticklabels=emotions, yticklabels=emotions, ax=axes[0])\n",
    "axes[0].set_title(f'Confusion Matrix - {best_model_name}\\n(Raw Counts)', fontsize=14)\n",
    "axes[0].set_xlabel('Predicted Emotion')\n",
    "axes[0].set_ylabel('Actual Emotion')\n",
    "\n",
    "# Normalized confusion matrix\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "sns.heatmap(cm_normalized, annot=True, fmt='.2f', cmap='Blues', \n",
    "           xticklabels=emotions, yticklabels=emotions, ax=axes[1])\n",
    "axes[1].set_title(f'Normalized Confusion Matrix - {best_model_name}\\n(Percentages)', fontsize=14)\n",
    "axes[1].set_xlabel('Predicted Emotion')\n",
    "axes[1].set_ylabel('Actual Emotion')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-class performance analysis\n",
    "print(\"=== Per-Class Performance Analysis ===\")\n",
    "\n",
    "# Calculate per-class metrics\n",
    "class_report = classification_report(y_test, y_pred_best, output_dict=True)\n",
    "\n",
    "class_metrics = []\n",
    "for emotion in emotions:\n",
    "    if emotion in class_report:\n",
    "        metrics = class_report[emotion]\n",
    "        class_metrics.append({\n",
    "            'Emotion': emotion,\n",
    "            'Precision': metrics['precision'],\n",
    "            'Recall': metrics['recall'],\n",
    "            'F1-Score': metrics['f1-score'],\n",
    "            'Support': metrics['support']\n",
    "        })\n",
    "\n",
    "class_metrics_df = pd.DataFrame(class_metrics)\n",
    "print(class_metrics_df.round(3))\n",
    "\n",
    "# Find best and worst performing emotions\n",
    "best_emotion = class_metrics_df.loc[class_metrics_df['F1-Score'].idxmax(), 'Emotion']\n",
    "worst_emotion = class_metrics_df.loc[class_metrics_df['F1-Score'].idxmin(), 'Emotion']\n",
    "best_f1 = class_metrics_df['F1-Score'].max()\n",
    "worst_f1 = class_metrics_df['F1-Score'].min()\n",
    "\n",
    "print(f\"\\nBest classified emotion: {best_emotion} (F1: {best_f1:.3f})\")\n",
    "print(f\"Most challenging emotion: {worst_emotion} (F1: {worst_f1:.3f})\")\n",
    "\n",
    "# Visualize per-class performance\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "x = np.arange(len(emotions))\n",
    "width = 0.25\n",
    "\n",
    "bars1 = ax.bar(x - width, class_metrics_df['Precision'], width, label='Precision', alpha=0.8)\n",
    "bars2 = ax.bar(x, class_metrics_df['Recall'], width, label='Recall', alpha=0.8)\n",
    "bars3 = ax.bar(x + width, class_metrics_df['F1-Score'], width, label='F1-Score', alpha=0.8)\n",
    "\n",
    "ax.set_xlabel('Emotions')\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_title(f'Per-Class Performance Metrics - {best_model_name}')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(emotions, rotation=45)\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_ylim(0, 1.1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Interpretation and Feature Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance analysis (for applicable models)\n",
    "def get_top_features(model, vectorizer, class_names, n_features=10):\n",
    "    \"\"\"Extract top features for each class\"\"\"\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    \n",
    "    if hasattr(model, 'coef_'):\n",
    "        # For linear models (SVM, Logistic Regression)\n",
    "        coef = model.coef_\n",
    "        top_features = {}\n",
    "        \n",
    "        for i, class_name in enumerate(class_names):\n",
    "            # Get top positive features\n",
    "            top_indices = np.argsort(coef[i])[-n_features:]\n",
    "            top_features[class_name] = [\n",
    "                (feature_names[idx], coef[i][idx]) for idx in top_indices[::-1]\n",
    "            ]\n",
    "        \n",
    "        return top_features\n",
    "    \n",
    "    elif hasattr(model, 'feature_importances_'):\n",
    "        # For tree-based models (Random Forest)\n",
    "        importances = model.feature_importances_\n",
    "        top_indices = np.argsort(importances)[-n_features:]\n",
    "        \n",
    "        return {\n",
    "            'Overall': [\n",
    "                (feature_names[idx], importances[idx]) for idx in top_indices[::-1]\n",
    "            ]\n",
    "        }\n",
    "    \n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Analyze feature importance for interpretable models\n",
    "print(\"=== Feature Importance Analysis ===\")\n",
    "\n",
    "interpretable_models = ['SVM', 'Logistic Regression', 'Random Forest']\n",
    "\n",
    "for model_name in interpretable_models:\n",
    "    if model_name in trained_models:\n",
    "        print(f\"\\n--- {model_name} ---\")\n",
    "        model = trained_models[model_name]\n",
    "        top_features = get_top_features(model, vectorizer, emotions, n_features=5)\n",
    "        \n",
    "        if top_features:\n",
    "            for class_name, features in top_features.items():\n",
    "                print(f\"\\nTop features for {class_name}:\")\n",
    "                for feature, score in features:\n",
    "                    print(f\"  {feature}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error analysis - find misclassified examples\n",
    "print(\"=== Error Analysis ===\")\n",
    "\n",
    "# Get misclassified examples\n",
    "misclassified_mask = y_test != y_pred_best\n",
    "misclassified_indices = y_test[misclassified_mask].index\n",
    "\n",
    "print(f\"Total misclassified samples: {sum(misclassified_mask)} out of {len(y_test)}\")\n",
    "print(f\"Misclassification rate: {sum(misclassified_mask)/len(y_test):.2%}\")\n",
    "\n",
    "# Show some misclassified examples\n",
    "print(\"\\nSample Misclassifications:\")\n",
    "misclassified_examples = []\n",
    "\n",
    "for i, idx in enumerate(misclassified_indices[:10]):  # Show first 10\n",
    "    original_text = df_clean.loc[idx, 'text']\n",
    "    true_emotion = y_test.loc[idx]\n",
    "    pred_emotion = y_pred_best[list(y_test.index).index(idx)]\n",
    "    \n",
    "    misclassified_examples.append({\n",
    "        'Text': original_text,\n",
    "        'True': true_emotion,\n",
    "        'Predicted': pred_emotion\n",
    "    })\n",
    "    \n",
    "    print(f\"\\n{i+1}. Text: {original_text}\")\n",
    "    print(f\"   True: {true_emotion} | Predicted: {pred_emotion}\")\n",
    "\n",
    "# Create confusion patterns\n",
    "print(\"\\n=== Common Confusion Patterns ===\")\n",
    "confusion_pairs = {}\n",
    "for true_label, pred_label in zip(y_test[misclassified_mask], \n",
    "                                 [y_pred_best[list(y_test.index).index(idx)] \n",
    "                                  for idx in y_test[misclassified_mask].index]):\n",
    "    pair = f\"{true_label} → {pred_label}\"\n",
    "    confusion_pairs[pair] = confusion_pairs.get(pair, 0) + 1\n",
    "\n",
    "# Sort by frequency\n",
    "sorted_confusions = sorted(confusion_pairs.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"Most common misclassification patterns:\")\n",
    "for pattern, count in sorted_confusions[:5]:\n",
    "    print(f\"  {pattern}: {count} times\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Model Deployment and Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a prediction function\n",
    "def predict_emotion(text, model=None, vectorizer=None, preprocessor=None):\n",
    "    \"\"\"\n",
    "    Predict emotion for a given text\n",
    "    \n",
    "    Args:\n",
    "        text: Input text string\n",
    "        model: Trained model (default: best model)\n",
    "        vectorizer: Fitted vectorizer\n",
    "        preprocessor: Text preprocessor\n",
    "    \n",
    "    Returns:\n",
    "        dict: Prediction results with emotion and confidence scores\n",
    "    \"\"\"\n",
    "    if model is None:\n",
    "        model = trained_models[best_model_name]\n",
    "    \n",
    "    # Preprocess text\n",
    "    processed_text = preprocessor.preprocess(text)\n",
    "    \n",
    "    # Vectorize\n",
    "    text_vectorized = vectorizer.transform([processed_text])\n",
    "    \n",
    "    # Predict\n",
    "    prediction = model.predict(text_vectorized)[0]\n",
    "    \n",
    "    # Get probabilities if available\n",
    "    confidence_scores = {}\n",
    "    if hasattr(model, 'predict_proba'):\n",
    "        probabilities = model.predict_proba(text_vectorized)[0]\n",
    "        confidence_scores = dict(zip(emotions, probabilities))\n",
    "    else:\n",
    "        # Create dummy probabilities for models without predict_proba\n",
    "        confidence_scores = {emotion: 0.0 for emotion in emotions}\n",
    "        confidence_scores[prediction] = 1.0\n",
    "    \n",
    "    return {\n",
    "        'predicted_emotion': prediction,\n",
    "        'confidence_scores': confidence_scores,\n",
    "        'max_confidence': max(confidence_scores.values()),\n",
    "        'processed_text': processed_text\n",
    "    }\n",
    "\n",
    "# Test the prediction function\n",
    "print(\"=== Emotion Prediction Examples ===\")\n",
    "\n",
    "test_texts = [\n",
    "    \"I'm absolutely thrilled about winning this competition!\",\n",
    "    \"This situation is making me really frustrated and angry\",\n",
    "    \"I feel so lonely and depressed today\",\n",
    "    \"The meeting is scheduled for tomorrow at 2 PM\",\n",
    "    \"I'm terrified about what might happen next\",\n",
    "    \"Wow, I never expected this amazing surprise!\"\n",
    "]\n",
    "\n",
    "expected_emotions = ['Happy', 'Angry', 'Sad', 'Neutral', 'Fear', 'Surprise']\n",
    "\n",
    "correct_predictions = 0\n",
    "\n",
    "for i, text in enumerate(test_texts):\n",
    "    result = predict_emotion(text, vectorizer=vectorizer, preprocessor=preprocessor)\n",
    "    \n",
    "    predicted = result['predicted_emotion']\n",
    "    expected = expected_emotions[i]\n",
    "    is_correct = predicted == expected\n",
    "    \n",
    "    if is_correct:\n",
    "        correct_predictions += 1\n",
    "    \n",
    "    print(f\"\\n{i+1}. Text: {text}\")\n",
    "    print(f\"   Predicted: {predicted} | Expected: {expected} | {'✓' if is_correct else '✗'}\")\n",
    "    print(f\"   Confidence: {result['max_confidence']:.3f}\")\n",
    "    \n",
    "    # Show top 3 confidence scores\n",
    "    sorted_scores = sorted(result['confidence_scores'].items(), \n",
    "                          key=lambda x: x[1], reverse=True)\n",
    "    print(f\"   Top 3: {sorted_scores[0][0]}({sorted_scores[0][1]:.3f}), \"\n",
    "          f\"{sorted_scores[1][0]}({sorted_scores[1][1]:.3f}), \"\n",
    "          f\"{sorted_scores[2][0]}({sorted_scores[2][1]:.3f})\")\n",
    "\n",
    "accuracy_on_examples = correct_predictions / len(test_texts)\n",
    "print(f\"\\nAccuracy on examples: {accuracy_on_examples:.2%} ({correct_predictions}/{len(test_texts)})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive prediction interface\n",
    "def interactive_prediction():\n",
    "    \"\"\"\n",
    "    Interactive function for emotion prediction\n",
    "    (Note: In Jupyter, this would typically use widgets for better interaction)\n",
    "    \"\"\"\n",
    "    print(\"=== Interactive Emotion Prediction ===\")\n",
    "    print(\"Enter text to analyze emotions (type 'quit' to exit):\\n\")\n",
    "    \n",
    "    # For demonstration, we'll use predefined examples\n",
    "    demo_texts = [\n",
    "        \"I just got promoted at work, I'm so excited!\",\n",
    "        \"This traffic jam is driving me crazy!\",\n",
    "        \"I can't believe my favorite restaurant closed down\",\n",
    "        \"Please submit your report by end of day\",\n",
    "        \"I'm worried about the test results\",\n",
    "        \"What an unexpected plot twist in that movie!\"\n",
    "    ]\n",
    "    \n",
    "    print(\"Demo predictions:\")\n",
    "    for i, text in enumerate(demo_texts, 1):\n",
    "        result = predict_emotion(text, vectorizer=vectorizer, preprocessor=preprocessor)\n",
    "        \n",
    "        print(f\"\\n{i}. Input: {text}\")\n",
    "        print(f\"   Emotion: {result['predicted_emotion']}\")\n",
    "        print(f\"   Confidence: {result['max_confidence']:.1%}\")\n",
    "        \n",
    "        # Show confidence bar\n",
    "        bar_length = int(result['max_confidence'] * 20)\n",
    "        bar = '█' * bar_length + '░' * (20 - bar_length)\n",
    "        print(f\"   [{bar}] {result['max_confidence']:.1%}\")\n",
    "\n",
    "# Run interactive prediction\n",
    "interactive_prediction()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Model Persistence and Deployment Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best model and components\n",
    "import pickle\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "def save_model_package(model, vectorizer, preprocessor, metadata, filename_prefix='emotion_model'):\n",
    "    \"\"\"\n",
    "    Save complete model package for deployment\n",
    "    \"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    # Save model components\n",
    "    model_package = {\n",
    "        'model': model,\n",
    "        'vectorizer': vectorizer,\n",
    "        'preprocessor': preprocessor,\n",
    "        'metadata': metadata\n",
    "    }\n",
    "    \n",
    "    model_filename = f\"{filename_prefix}_{timestamp}.pkl\"\n",
    "    with open(model_filename, 'wb') as f:\n",
    "        pickle.dump(model_package, f)\n",
    "    \n",
    "    # Save metadata as JSON\n",
    "    metadata_filename = f\"{filename_prefix}_metadata_{timestamp}.json\"\n",
    "    with open(metadata_filename, 'w') as f:\n",
    "        json.dump(metadata, f, indent=2, default=str)\n",
    "    \n",
    "    return model_filename, metadata_filename\n",
    "\n",
    "# Prepare metadata\n",
    "metadata = {\n",
    "    'model_name': best_model_name,\n",
    "    'model_type': type(trained_models[best_model_name]).__name__,\n",
    "    'training_date': datetime.now().isoformat(),\n",
    "    'dataset_size': len(df_clean),\n",
    "    'emotions': emotions,\n",
    "    'performance': {\n",
    "        'accuracy': float(results[best_model_name]['accuracy']),\n",
    "        'precision': float(results[best_model_name]['precision']),\n",
    "        'recall': float(results[best_model_name]['recall']),\n",
    "        'f1_score': float(results[best_model_name]['f1_score']),\n",
    "        'cv_score': float(results[best_model_name]['cv_mean'])\n",
    "    },\n",
    "    'vectorizer_params': vectorizer.get_params(),\n",
    "    'feature_count': X_train_vectorized.shape[1],\n",
    "    'class_distribution': y.value_counts().to_dict()\n",
    "}\n",
    "\n",
    "# Save the model package\n",
    "model_file, metadata_file = save_model_package(\n",
    "    trained_models[best_model_name], \n",
    "    vectorizer, \n",
    "    preprocessor, \n",
    "    metadata\n",
    ")\n",
    "\n",
    "print(\"=== Model Package Saved ===\")\n",
    "print(f\"Model file: {model_file}\")\n",
    "print(f\"Metadata file: {metadata_file}\")\n",
    "print(f\"Best model: {best_model_name}\")\n",
    "print(f\"Accuracy: {results[best_model_name]['accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create deployment instructions\n",
    "deployment_instructions = f\"\"\"\n",
    "# Emotion Detection Model - Deployment Instructions\n",
    "\n",
    "## Model Information\n",
    "- **Model**: {best_model_name}\n",
    "- **Accuracy**: {results[best_model_name]['accuracy']:.4f}\n",
    "- **F1-Score**: {results[best_model_name]['f1_score']:.4f}\n",
    "- **Training Date**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "- **Dataset Size**: {len(df_clean)} samples\n",
    "\n",
    "## Quick Start\n",
    "\n",
    "```python\n",
    "import pickle\n",
    "\n",
    "# Load the model package\n",
    "with open('{model_file}', 'rb') as f:\n",
    "    package = pickle.load(f)\n",
    "\n",
    "model = package['model']\n",
    "vectorizer = package['vectorizer']\n",
    "preprocessor = package['preprocessor']\n",
    "\n",
    "# Make predictions\n",
    "def predict_emotion(text):\n",
    "    processed = preprocessor.preprocess(text)\n",
    "    vectorized = vectorizer.transform([processed])\n",
    "    prediction = model.predict(vectorized)[0]\n",
    "    \n",
    "    if hasattr(model, 'predict_proba'):\n",
    "        probabilities = model.predict_proba(vectorized)[0]\n",
    "        confidence = max(probabilities)\n",
    "    else:\n",
    "        confidence = 1.0\n",
    "    \n",
    "    return prediction, confidence\n",
    "\n",
    "# Example usage\n",
    "emotion, confidence = predict_emotion(\"I'm so happy today!\")\n",
    "print(f\"Emotion: {{emotion}}, Confidence: {{confidence:.3f}}\")\n",
    "```\n",
    "\n",
    "## Performance Summary\n",
    "- Emotions supported: {', '.join(emotions)}\n",
    "- Best performing emotion: {best_emotion} (F1: {best_f1:.3f})\n",
    "- Most challenging emotion: {worst_emotion} (F1: {worst_f1:.3f})\n",
    "- Feature count: {X_train_vectorized.shape[1]}\n",
    "\n",
    "## Requirements\n",
    "- Python 3.7+\n",
    "- scikit-learn\n",
    "- pandas\n",
    "- numpy\n",
    "- nltk\n",
    "\n",
    "## Notes\n",
    "- Text preprocessing includes cleaning, tokenization, and lemmatization\n",
    "- TF-IDF vectorization with 1-2 grams\n",
    "- Model trained on balanced dataset with {len(df_clean)} samples\n",
    "\"\"\"\n",
    "\n",
    "# Save deployment instructions\n",
    "instructions_file = f\"deployment_instructions_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md\"\n",
    "with open(instructions_file, 'w') as f:\n",
    "    f.write(deployment_instructions)\n",
    "\n",
    "print(f\"\\nDeployment instructions saved to: {instructions_file}\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(deployment_instructions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary and Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print(\"=\" * 80)\n",
    "print(\"EMOTION DETECTION PROJECT SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\n📊 DATASET:\")\n",
    "print(f\"   • Total samples: {len(df_clean)}\")\n",
    "print(f\"   • Emotions: {len(emotions)} classes ({', '.join(emotions)})\")\n",
    "print(f\"   • Average text length: {df['text_length'].mean():.1f} characters\")\n",
    "print(f\"   • Dataset balance: {'Balanced' if balance_check < 0.1 else 'Imbalanced'}\")\n",
    "\n",
    "print(f\"\\n🔧 PREPROCESSING:\")\n",
    "print(f\"   • Text cleaning and normalization\")\n",
    "print(f\"   • Tokenization and lemmatization\")\n",
    "print(f\"   • TF-IDF vectorization (1-2 grams)\")\n",
    "print(f\"   • Feature count: {X_train_vectorized.shape[1]}\")\n",
    "\n",
    "print(f\"\\n🤖 MODELS TRAINED:\")\n",
    "for name, result in results.items():\n",
    "    print(f\"   • {name}: {result['accuracy']:.3f} accuracy, {result['training_time']:.2f}s training\")\n",
    "\n",
    "print(f\"\\n🏆 BEST MODEL: {best_model_name}\")\n",
    "print(f\"   • Accuracy: {results[best_model_name]['accuracy']:.4f}\")\n",
    "print(f\"   • Precision: {results[best_model_name]['precision']:.4f}\")\n",
    "print(f\"   • Recall: {results[best_model_name]['recall']:.4f}\")\n",
    "print(f\"   • F1-Score: {results[best_model_name]['f1_score']:.4f}\")\n",
    "print(f\"   • CV Score: {results[best_model_name]['cv_mean']:.4f} ± {results[best_model_name]['cv_std']:.4f}\")\n",
    "\n",
    "print(f\"\\n📈 CLASS PERFORMANCE:\")\n",
    "print(f\"   • Best emotion: {best_emotion} (F1: {best_f1:.3f})\")\n",
    "print(f\"   • Most challenging: {worst_emotion} (F1: {worst_f1:.3f})\")\n",
    "print(f\"   • Average F1-Score: {class_metrics_df['F1-Score'].mean():.3f}\")\n",
    "\n",
    "print(f\"\\n🎯 DEPLOYMENT:\")\n",
    "print(f\"   • Model saved: {model_file}\")\n",
    "print(f\"   • Metadata: {metadata_file}\")\n",
    "print(f\"   • Instructions: {instructions_file}\")\n",
    "print(f\"   • Ready for production use\")\n",
    "\n",
    "print(f\"\\n✅ PROJECT COMPLETE!\")\n",
    "print(f\"   • End-to-end ML pipeline implemented\")\n",
    "print(f\"   • Multiple models trained and evaluated\")\n",
    "print(f\"   • Best model selected and deployed\")\n",
    "print(f\"   • Comprehensive analysis and documentation\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps and Improvements\n",
    "\n",
    "### Potential Enhancements:\n",
    "\n",
    "1. **Advanced Models**\n",
    "   - BERT/RoBERTa for better context understanding\n",
    "   - Ensemble methods combining multiple models\n",
    "   - Deep learning approaches (LSTM, GRU)\n",
    "\n",
    "2. **Feature Engineering**\n",
    "   - Sentiment lexicon features\n",
    "   - POS tagging and syntactic features\n",
    "   - Word embeddings (Word2Vec, GloVe)\n",
    "\n",
    "3. **Data Expansion**\n",
    "   - Larger datasets (Twitter, Reddit, reviews)\n",
    "   - Multi-language support\n",
    "   - Domain-specific emotion detection\n",
    "\n",
    "4. **Production Features**\n",
    "   - Real-time API deployment\n",
    "   - Model monitoring and retraining\n",
    "   - A/B testing framework\n",
    "   - Explainable AI features\n",
    "\n",
    "5. **Evaluation**\n",
    "   - Human evaluation studies\n",
    "   - Cross-domain validation\n",
    "   - Bias and fairness analysis\n",
    "\n",
    "### Applications:\n",
    "\n",
    "- **Customer Service**: Automatic emotion detection in support tickets\n",
    "- **Social Media**: Brand sentiment monitoring\n",
    "- **Mental Health**: Mood tracking applications\n",
    "- **Education**: Student engagement analysis\n",
    "- **Marketing**: Emotional response to campaigns\n",
    "\n",
    "---\n",
    "\n",
    "*This notebook demonstrates a complete machine learning pipeline for emotion detection, from data exploration to model deployment. The systematic approach ensures reproducibility and provides a solid foundation for production deployment.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}